# -*- coding: utf-8 -*-
"""Geolocalizacao.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oUXol60k1687gX3yeITrS7dAEEO1gzOA
"""

#!pip install pyspark

#Import de bibliotecas

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.window import Window
import pandas as pd
import numpy as np
#import pyspark.pandas as ps
import string
import seaborn as sns
import folium
from matplotlib import pyplot as plt

#Spark Session e opções panda e de caminho do arquivo

spark = SparkSession.builder.appName("AnalizeGeoLocalizacao").getOrCreate()

pd.options.display.html.table_schema=True
path = "/content/drive/MyDrive/"

v = spark.read.parquet(path + "/Votacao/completa").repartition("cidade")
c = spark.read.csv(path + "/Votacao/municipios.csv", inferSchema=True, sep = ",", header=True)
e = spark.read.csv(path + "/Votacao/estados.csv", inferSchema=True, sep = ",", header=True)

from google.colab import drive
drive.mount('/content/drive')

c.createOrReplaceTempView("c")
e.createOrReplaceTempView("e")
v.createOrReplaceTempView("v")

#ce = spark.sql("SELECT uf, e.latitude as uflat, e.longitude as uflon, \
#                    UPPER(c.nome) as cidade, c.latitude as clat, c.longitude as clon\
#                    FROM c INNER JOIN e ON c.codigo_uf = e.codigo_uf")
#ce.write.parquet(path + "/Votacao/latlonBR", mode="overwrite")


ce = spark.read.parquet(path + "/Votacao/latlonBR").repartition("cidade")
ce.createOrReplaceTempView("ce")

#vc = spark.sql ("SELECT v.*, uflat, uflon, clat, clon \
#                    FROM v \
#                    INNER JOIN ce ON (v.cidade = ce.cidade and v.uf = ce.uf)")


#vc.write.parquet(path + "/Votacao/vc", mode="overwrite")

vc = spark.read.parquet(path + "/vc")
vc.createOrReplaceTempView("vc")

vc.printSchema()

sdf = vc.selectExpr("lula / aptos as lula","bolsonaro / aptos as bolsonaro",\
                          "lula as lula_votos", "bolsonaro as bolsonaro_votos", \
#                          "tebet as tebet_votos", "tebet / aptos as tebet",\
#                          "ciro as ciro_votos", "ciro / aptos as ciro", \
#                          "soraya as soraya_votos", "soraya / aptos as soraya",\
#                          "davila as davila_votos", "davila / aptos as davila", \
                          "nulos as nulos_votos", "nulos / aptos as nulos", \
                          "brancos as brancos_votos", "brancos / aptos as brancos", 
                        "uf", \
                        "P_DIR / aptos as p_dir", "P_ESQ / aptos as p_esq",\
                        "G_DIR / aptos as g_dir", "G_ESQ / aptos as g_esq",\
                        "S_DIR / aptos as s_dir", "S_ESQ / aptos as s_esq",\
                        "RAZAO_BOLSO_LULA as bl", "RAZAO_LULA_BOLSO as lb",\
                           "regiao",\
                           "CASE regiao WHEN 'N' THEN 1 WHEN 'NE' THEN 2 WHEN 'SE' THEN 3 WHEN 'S' THEN 4 WHEN 'CO' THEN 5 WHEN 'ZZ' THEN 6 END as regiao_id",\
                           "cidade", "urna", "abstencoes", "comparecimento", \
                           "abstencoes / aptos as tx_abstencoes",\
                           "nao_habilitados_biometria / aptos as nhb", "data_bu", "zona", \
                           "secao", "CAST(LOG_MODELO as string) as LOG_MODELO_STR", "CAST(LOG_MODELO as double) as LOG_MODELO",\
                           "flashcard_urna", "clat", "clon", "divergencia")\
                        .orderBy("data_bu")\
                        .fillna("N/D", ["divergencia"])

sdf = spark.sql("SELECT avg(lula / aptos) as lula, avg(bolsonaro / aptos) as bolsonaro, avg(lula / aptos) - avg(bolsonaro / aptos) as luladiff,\
                          if (sum(lula) > sum(bolsonaro), 1, 0) as lula_won,\
                          sum(lula) as lula_votos, sum(bolsonaro) as bolsonaro_votos,\
                          sum(nulos) as nulos_votos, avg(nulos / aptos) as nulos, \
                          sum(brancos) as brancos_votos, avg(brancos / aptos) as brancos,\
                        first(uf) as uf, \
                        avg(P_DIR / aptos) as p_dir, avg(P_ESQ / aptos) as p_esq,\
                        avg(G_DIR / aptos) as g_dir, avg(G_ESQ / aptos) as g_esq,\
                        avg(S_DIR / aptos) as s_dir, avg(S_ESQ / aptos) as s_esq,\
                        first(regiao) as regiao,\
                        first(cidade) as cidade,  \
                        avg(abstencoes / aptos) as tx_abstencoes,\
                        avg(nao_habilitados_biometria / aptos) as nhb, \
                        first(clat), first(clon) FROM vc GROUP BY cidade, uf")


#                          "tebet as tebet_votos", "tebet / aptos as tebet",\
#                          "ciro as ciro_votos", "ciro / aptos as ciro", \
#                          "soraya as soraya_votos", "soraya / aptos as soraya",\
#                          "davila as davila_votos", "davila / aptos as davila", \

sdf.groupBy("lula_won").agg(avg("luladiff")).toPandas()

#Detecção de anomalias com K-Means ou Gaussian Mixture
#Feature Engineering

#from pyspark.ml.feature import Binarizer
#binarizer = Binarizer(inputCol="LOG_MODELO", outputCol="Urna_nova", threshold = 2016)
#sdf_bin = binarizer.transform(sdf)



from pyspark.ml.feature import StringIndexer
indexer = StringIndexer(inputCol="divergencia", outputCol="divergencia_id")
indexer_model = indexer.fit(sdf_bin)
list(enumerate(indexer_model.labels))
indexed = indexer_model.transform(sdf_bin)

#features = ["bolsonaro", "lula", "clat", "clon", "divergencia_id"]
features = ["bolsonaro", "lula", "clat", "clon"]


from pyspark.ml.feature import VectorAssembler
va = VectorAssembler(inputCols=features, outputCol="features")
features_assembled = va.transform(indexed)

from pyspark.ml.feature import StandardScaler
ss = StandardScaler(inputCol="features", outputCol="features_scaled", withMean=False, withStd=True)
features_scaled = ss.fit(features_assembled).transform(features_assembled)

#Gaussian Mixture 

from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import GaussianMixture
gm = GaussianMixture(featuresCol="features_scaled", k=4, seed=12345)

# Examine the hyperparameters:
print(gm.explainParams())

# Fit the Gaussian mixture model:
gmm = gm.fit(features_scaled)
type(gmm)

predictions = gmm.summary.predictions

def plot_clusters(df):
  # Specify a color palette:
  colors = ["blue", "orange", "green", "red"]
 
  m = folium.Map(location=[-15.83, -47.86], zoom_start=4)

  rows = df.sample(withReplacement=False, fraction=0.1, seed=42)\
        .select("clat", "clon", "lula", "prediction").collect()
  for row in rows:
    folium.CircleMarker(location=[row["clat"], row["clon"]], radius=row["lula"]*23, color=colors[row["prediction"]], fill=True).add_to(m)
  # Add the cluster centers (Gaussian means):
#  centers = gmm.gaussiansDF.collect()
#  for (i, center) in enumerate(centers):
#    folium.CircleMarker(location=center["mean"], color=colors[i]).add_to(m)
  # Return the map:
  return(m)

m = plot_clusters(predictions)

m

predictions.crosstab("prediction", "divergencia").show()

